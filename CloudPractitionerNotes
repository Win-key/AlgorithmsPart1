1/17/2022
=========
Types of cloud
1. Private cloud
    Used by single organization.
    Complete control over network
    More security
    eg - Rackspace

2. Public cloud
    Cloud resources owned by third parties
    Delivered over internet

3. Hybrid cloud
    Can have both public clould and on primise

Adv:
1. On demand seft service
2. Broad network - Can access diverse platform and services over internet
3. Multi - tenancy and resource pooling - Multiple user can use the same infra
4. Rapid elasticity and scalability - Automatic & Quick scale up and down on demand
5. Measured service - You pay for what you use

Six advantages of cloud computing
1. Pay on demand
2. Low economies - as AWS run in large scale serving multiple users
3. Stop guessing capacity - Can get/remove resource on demand
4. Increase speed
5. Maintenance of servers is avoided
6. Go global in a min

Type of cloud computing
1. Infra as a service
    Provide the computers, networks , storage space
2. Platform as a service
    Taking the responsibility of managing the infra away from the organization
3. Software as a service
    Complete product that runs and managed by the cloud provider


On-Premises      Iaas              Paas               Saas
===========      ====              ====               ====
Applications     Applications      Applications       * Applications
Data             Data              Data               * Data
Runtime          Runtime           * Runtime          * Runtime
Middleware       Middleware        * Middleware       * Middleware
O/S              O/S               * O/S              * O/S
Virtualization   * Virtualization  * Virtualization   * Virtualization
Server           * Server          * Server           * Server
Storage          * Storage         * Storage          * Storage
Networking       * Networking      * Networking       * Networking

* Managed by provider

Types of Pricing
================
Incoming data is free
3 pricing model
1. Pay for computation
2. Pay for storage used
3. Pay for outgoing data

IAM
===========================================

Users and Groups
========================================================
Identity and Access Management
Global service
Root account - only for set up. should not be used for other purposes.
Users can be created with in an account.
Group is a collection of Users. Eg: Developers - Which may contains the users belong to development
Users can belongs to multiple groups.

** It's not best practice that users not added to a group.
    Access should be provided via groups. Not individually.

IAM - Permission
========================================================
Policy - A json data - That defines the access of a service and resource.
Policy can be assigned to a group or an individual user.

** Best practice: Least Privilege Access - Don't give more permission than user need.



Policy Inheritance
==========================================================

      developer policy            audit policy               testing policy         individual
        |                           |                           |                       |
    dev group                   audit group                 QA group                    |
________|_____  __________________|______________  ___________|____  ___________________|
|       |    | |                                | |      |        | |
dev 1 dev 2 dev 3                               qa 1    qa 2      qa3


Policy structure
===============
version - version of the language like 2012-10-17
id - identifier of the policy - opt
Statement - List of object that defines the access

Statement consist of
====================
sid - id of the statement - opt
effect - Allow or Deny
principle - account/user/role to which the policy need to be applied
action - List of actions this policy allows/denies
resource - List of resources to which these actions applied
condition - condition on which the policy is effective


Eg:
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "ListObjectsInBucket",
            "Effect": "Allow",
            "Action": ["s3:*"],
            "Resource": ["arn:aws:s3:::cdsoa*"]
        },
        {
            "Sid": "AllObjectActions",
            "Effect": "Allow",
            "Action": "s3:*Object",
            "Resource": ["arn:aws:s3:::bucket-name/*"]
        }
    ]
}


IAM - Password Policy
================================================================================
Strong password
Define rules like password should have min length of password , upper / lower letters, numbers, special chars
Password expiration
Prevent password reuse

Go to
    IAM -> Account Settings -> Change password policy


IAM - MFA : Multi Factor Authentication
================================================================================
Advised to protect the root account with MFA, but can be done for all IAM users as well.
Combination of password and token generated from Authorized device.

How to access AWS
=================
* AWS Management Console - Protected by Password and MFA - provides access from computer browser
* AWS CLI - Protected by access keys and secret keys - provides access from local computer - Requires CLI installation - Build using Aws SDK for Python
* AWS SDK - Protected by access keys and secret keys - provides access from software application

Never share the access key and secret. They are important as password, since they provide access to your aws account and resources

To Start - AWS CloudShell: Region Availability

Cloud shell
===========

Allows users to run the aws commands from the browser.
Any files created here will stay here even after the cloud shell restart
Can create multiple command line within the cloud shell

IAM roles for AWS services
==========================

IAM roles are secure ways to grant permission to other entities that you trust.

AWS services sometime need permission to talk with other services
Like Ec2 talks with S3 to read and write files
Do achieve that we will have roles assigned to AWS services

Common roes:
* Ec2 instance role
* Lambda functions role
* Roles for cloud formations

How to access it -> IAM -> Roles -> Create / edit roles

IAM security tools
==================

1. IAM Credential report (account level) -
    All account's users and their status of various credentials

    IAM -> Under Access reports -> Credentials report

2. IAM Access Advisor (User level) -
    Service permissions granted to the users and the last accessed date of service
    So that we can revise the policies
    Helps to ensure the least privilege access

    IAM -> Users -> Click on a user -> Click Access advisor tab

IAM Best practices
==================

* Never use root account except for setting up the account
* One IAM user = One physical person
* Assign users to group for permissions
* Strong password policy
* Enable MFA
* Access keys and secret keys should be kept safe and not be shared with anyone.
* Use IAM roles for assigning permissions to AWS services
* User credential reports and access advisor to audit the users policy to ensure the least privilege access

Summary
========
Users : IAM user mapped to a physical person in the company
Group : Group of users -> Permissions assigned to the groups inherited to the users of the groups
        A user can be associated with multiple groups
Policy : A json document that define the permission which can be assigned to the user or a group


===========================================================
=======================  EC 2  ============================
===========================================================

Budget management
=================
Account -> My billing dashboard
This special permission need to be provided by the root account user
An IAM user to access this dashboard, One should log in as root user and
in My account need to enable IAM users to access Billing dashboard

To create a budget, Click on Budget tab in side nav bar.
    Choose a budget type
        Cost saving budget - Plan how much you want to spend on a service
        Usage budget       - Plan how much you want to use one or more services.
        Saving plan budget - Define a utilization threshold and receive alerts when the usage of your Savings Plans falls below that threshold.
        Reservation budget -

    These budgets help to configure the alerts when the threshold reaches.
    And send out the email alerts to the users.

EC2 Basics
===========
Elastic Cloud Computing - Infra as a service - IAAS
* Renting Virtual machines  - EC2
* Storage volumes           - EBS
* Load distributions        - ELB
* Auto scaling              - ASG

EC2 sizing and config options
=============================
OS - Linux , windows and Mac
CPU cores
RAM
Storage
    Network storage - EBS(Elastic block store)
                    - EFS (Elastic file system)

    Hardware        - EC2 instance store
Network card - speed and IP addresses
Fire wall using security groups
User data using bootstrap script script

EC2 User data
==============
Lunching instance with script
    which will install os updates , software
    and anything that needed for setting up the computer(EC2 here)

It's executed only once when instance is started at first time.
It will run with the root user. (sudo rights)

Few instance types:
    t2.micro
    t2.xlarge
    c5d.4xlarge
    r5....
    m5...

Creating AWS EC2 instances
==========================
1. Select an Instance
    Amazon Machine Image - Image that used to launch the machine

    Quick start - Provide AWS AMI
    My Images   - AMIs created by us
    Market place- AMIs created by other people
    Community   - Created by community people
2. Choose instance type - t2.micro, large, xlarge, ...
3. Instance details -
    Number of instances
    network
    Ip address
    Assigning IAM roles
    ** User data script - Can be provided as text or a file in based 64 encoded
        https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html#user-data-shell-scripts
4. Add storage
    -
5. Tags
6. Security group - Can create or select from existing security groups
7. Review and launch
8. Create key value pair
    - Key to access the ec2 instance - Can download only once
    - Can create a new pair or select an existing pair
9. Launch....

How to end
    Stop - Stop for temp. Can start again.
    Terminate - Once terminate, cannot start the same instance again.
                Need to create a new instance.

======================================================
=================EC3 Instance class===================
======================================================

https://aws.amazon.com/ec2/instance-types/

Naming conventions: -
    m5:2xlarge
m       - instance class - defines the class of the instance - General purpose, storage or other type
5       - generation of the instance
2xlarge - size in the instance class

Instance classes
================
General Purpose : -
                provide a balance of compute, memory and networking resources
                Suitable for web servers and code repos

Compute Optimized -
            Used for tasks that requires high performance
            Like batch processing
            High performance web servers
            High performance computing
            Machine learnings

Memory Optimized
            Can handle huge volumn of data in memory
            Like SQL and NO-SQL dbs requires high performance and should be capable of handing huge data in memory
            Cache storages
            Real time unstructured data processing
Storage Optimized
            Great for high intensive file read write tasks
            Like SQL and NO-SQL dbs requires high performance and should be capable of handing huge data in memory
            Cache storages
            Real time unstructured data processing

Accelerated Computing
Instance Features
Measuring Instance Performance

=======================================================
==================== Security groups ==================
=======================================================

Fire wall to the ec2 instances
Defines the in and out bound network rules of the ec2 instances
Only specifies what ips can be allowed.
Can be tied with other Security groups
Access to ports

Where to create/edit SG
=======================
EC2 instance -> Network -> Security groups

Things to know:
===============
* Can be attached to multiple instances
* Locked to a region and vpc combinations
* Security group is not part of ec2 and that lives outside of ec2
    If the Security group blocks a request,
    ec2 is completely unaware
    that there is a request blocked by the security group
* Good to have on security group for SSH
* Time out exception - Possibly security group
* Connection refused -
     NO Security group issue, request reaches the ec2
     but there is other issue within ec2 that throws connection refused.
* All in bound requests are blocked
* All out bound requests are authorised
* You can apply multiple security groups to a single EC2 instance or apply a single security group to multiple EC2 instances

*** A security group can refer other security groups and allow based on the rules of referred security groups
    Doubt:
    ======
     * I have 2 security groups
        SG1 and SG2
        1. SG1 -> IB
            TLS -> 1122 -> SG2
        2. SG2 -> IB
            TLS -> 1122 -> IP range
            --> Should I need to have the same Protocol and Port here??

Importance ports to know
========================

22 - SSH - Secure shell - Log in to linux instance
21 - FTP - File Transfer Protocol - Upload files into file system
22 - SFTP - Secured FTP - Secured way of Uploading files into file system
80 - http - Hyper text transfer protocol - access UNsecured websites
443 - https - Hyper text transfer protocol Secured - access secured websites
3389 - RDP - Remote Desktop Protocol - log into windows system

How to connect to EC2 machine to do maintenance?
===============================================

                 SSH     Putty       Ec2 instance connect
                ------------------------------------------
Linux          | Yes   |  NA     |     Yes
MAc            | Yes   |  NA     |     Yes
Windows < 10   | NO    |  Yes    |     Yes
Windows >=10   | Yes   |  Yes    |     Yes

Ec2 instance connect works only for Amazon Linux 2 AMI

Hands on SSH
============

Using Mac/Linux/Windows 10+
===========================

We need the public ip of the EC2 machine
And security group that allows the user's IP over ssh protoco
    ssh -> 22 -> 0.0.0.0/0
And the key file (.pem)

ssh ec2-user@public_ip_of_ec2_machine

ec2-user is the default user name of ec2

==> Should be a Permission denied response
Though SSH configured to allow all IP,
   it requires the EC2 key pair file (.pem) to authorize the user.

ssh -i ec2_key_pair.pem ec2-user@public_ip_of_ec2_machine
    (ec2_key_pair.pem file should be placed in the directory)

If the key file is unprotected with chmod 0644 command,
    then there should be another Permission Denied response

To fix this, run
    chmod 0644 ec2_key_pair.pem

Should be good to go now........

To exit, run exit or ctrl + C

How to ssh into EC2 from Window using Putty
============================================
https://www.youtube.com/watch?v=jv-dgOfFN4o

Ec2 Instance connect
====================
Connect over browser.
This also requires SSH - 22 to be configured in SG

EC2 IAM roles
=============
When we need to have access to IAM role,
    we have to assign a IAM role to the EC2 instance.

We have already seen how to create an IAM role.
To assign it to an instance.
EC2
    -> Select an instance
        -> Actions
            -> Security
                -> Modify IAM role
                    -> Now already created IAM roles should be shown now.
                        Select the role needed
                        -> Save. Now permissions assigned to IAM role should be available for EC2 too.

How to connect to windows instance over RDP
===========================================

https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/connecting_to_windows_instance.html

EC2 Instance Launch types
=========================

1. On Demand
=============
* Pay for what you use
* Billing per second, after first minute (free)
* High cost, but No upfront payment (advance amount)
* Can terminate at any time

Best for short time and uninterrupted workloads

Reserved instances
==================
* Up to 75% discount compared to On demand
* Reservation can be 1 year or 3 year
* Purchase options
    - No upfront advance
    - Partial upfront advance + discount
    - Full payment ++ discount
* Can reserve a specific instance type
* Recommended for steady state instance - database

Other Variants of Reserved instances
===================================
Convertible Reserved instances
==============================
* Allows to convert the instances type
* Up to 54% discount (Lesser discount than the normal reserved instances)
Scheduled Reserved Instances
============================
* When having daily jobs that runs once in a day/month/year likewise
* Reserve for the time window
* Can reserve for 1 to 3 years
* Currently deprecated. We cannot buy now.

Spot instances
==============
* Can get 90% discount compared to on demand
* Can lose the instance at any point of time when the MAX threshold is reached
* Most cost efficient AWS instance type
* Should be used for recoverable use cases - Batch job that can be rerun, Image processing, distributed workloads(LB)
* Never use for critical tasks - DB, web server

Dedicated Host
==============
* Physical server with Ec2 instances which are fully dedicated our use
* Can address Compilence requirement
* Only allocated for 3 years reservation
 * Most expensive
* Since the physical hardware are dedicated to us, we can make use of the server inbound software licenses,
    Which will reduce our licensing costs.
        Eg: Windows license, SQL server license and other server licenses that are tied to vm, cpu and physical cores..
* This enable us to bring-your-own-license (BYOL). Used for complex licensing model.
* Provide more control of the underlying hardware like visibility of sockets, vms,cores, host ip.
    Targeted instance placement
https://aws.amazon.com/ec2/dedicated-hosts/
https://aws.amazon.com/about-aws/whats-new/2015/11/now-available-amazon-ec2-dedicated-hosts-and-the-ability-to-use-existing-server-bound-licenses/

Dedicated Instances
===================
* Instances running on hardware that is dedicated for us
* Instances can share the hardware within the same account.
    But not with other.
* Automatic instance placement only. No target placement
    Means first time intance could be placed on HW1, next time it could be HW@

Comparing Dedicated Host and Dedicated Instances
================================================
https://aws.amazon.com/ec2/dedicated-hosts/#:~:text=An%20important%20difference%20between%20a,same%20physical%20server%20over%20time.

Shared Responsibility between AWS and User on EC2
=================================================
            AWS                             User
* Infra network security                 * Security group rules
* Isolation of Physical host             * OS & software patches and updates
* Faulty HW replacement                  * IAM roles and IAM user access managements
* Compliance                             * Data security on EC2

Ec2 Instance Store
==================
* Storage HW device witnin the host machine.
* Provides low latency and high throughput
* Selecting Instance store or EBS is based on instance type.
* it offers temporary storage. In case of any Instance Failure, you will lose your data.
* Can be attached only during the start
* There is no stop for Instance store EC2. Only termination
* After you launch an instance, you must ensure that the instance store volumes for your instance are formatted and mounted before you can use them.

Elastic Block Store
===================

* It's a network drive attached to the EC2 instance while they run.
    - Since they are attached over network. Network Latency will be there.
    - They can be unmounted from a machine and mounted to another machine.
* It allows to persist the data, even after the EC2 termination
* It's bounded to the availability zone.
    - cannot mount a EBS from az1 to az2's ec2
    - To move the volume, you need to snapshot.
* At a time, it can be attached to the one instance alone.(At CCP level)
* Configured capacity will be provided.(GB and IOPS)
    - Billed what capacity and IOPS
    - Can change the capacity over time if needed.
* Multiple EBS volumes can be attached to a ec2 instance
* EBS volume and ec2 has to be in same az.
* For Failure recovery, EBS volume is auto replicate within the az.

Delete on termination policy
============================
There is a property in the create EC2 instance -> configure storage named "Delete on Termination"
This property allows the user to decide whether to delete the storage on instance termination
* Root volume
    - If it's instance store, volume will be delete on termination (NA)
    - If it's EBS volume, by default it will be delete on termination
        But can be opted not to delete on termination
* Non root storage
    - EBS volumes, By default not to delete on termination. but can be changed
    - Instance store also can be used as non root volume.


Good read for root volume to understand difference between EBS backed instance and Instance store backed instance
https://faun.pub/aws-understanding-root-device-volume-76df89d18ec4

EBS Multi-Attach
================
* io1 and io2 volume types can be attached to multiple instances
    For ccp they are out scoped.

EBS Snapshots
=============
* Back up of EBS volume at a point of time
* It's recommended to take the snapshot after detaching it from ec2, but not necessary.
* The state of EBS volume is persisted in snapshot and so can be used to create/recreate another ec2 from snapshot
* This snapshot can be copied to other AZ/Region and can be used for creating EC2 there.

Step to create snapshot
Select EBS volume
    -> Action -> create snapshot -> Add description
        -> Create snapshot

This created snapshot can be found in
    EBS service -> Snapshot in the left side nav bar

Copy snapshot
============
Select snapshot -> action -> copy
    -> select region
    -> copy

Create volume
=============
Select snapshot -> action -> Create volume
    -> Value are Pre populated with origin volume
    -> can change az here
    -> Create volume

Now this new volume can be attached to another az's EC2 instance.

AMI overview
=============
* Amazon Machine Image
* Customization of Ec2 instance
    - Can add our OS, software, configs, monitoring tools
    - Faster boot - As the software, configurations are pre-packaged.
* Build for specific region, can be copied to other region
* Options
    - Public AMI  - AWS provided
    - Own AMIs    - We make and maintain
    - MarketPlace - Someone else build and sell
    - Community   - Build by a community of people.
* Your AMI is stored as an EBS snapshot in the same region as the instance.
  You will pay for the cost of that EBS storage.

Types of AMIs
=============
Amazon EBS-backed AMI – The root device for an instance launched from the AMI is an Amazon Elastic Block Store (Amazon EBS) volume created from an Amazon EBS snapshot.
Amazon instance store-backed AMI – The root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3.

Create an AMI
=============
1. Need to start a EC2 first
    In User data script, add the bash script to install the softwares we need.
2. Can create AMI from either running instance or stopped instance
3. Right click on EC2 instance from where we want to create image.
4. Image name and create.
    - this will take a snapshot of ebs root volume
    - it will take a little bit of time to create a image.

Start an EC2 with My AMIs
=========================
1. Create Instance
2. My AMIs
3. Do regular steps
    - User data script only for business. Like running tomcat.
    - Because other things are taken care by base AMI that we have created.
4. Launch instance.
    - There should a faster boot up time as the software and configurations are pre-packed by the base AMI

EC2 Image builder
=================
Problem:
* Keeping Virtual Machine and container images up-to-date can be time consuming, resource intensive, and error-prone.
Currently, customers either manually update and snapshot VMs or have teams that build automation scripts to maintain images.

Solution:
* Ec2 image builder used to automate the building, testing and deployments of VM and container images.
* This is not for AWS EC2, also for on premise instances connected to aws.
* Can run on schedule (Weekly, monthly or on software updates)
* Free of cost - But need to pay for underlying resources
    - EC2s created during the Image building process
    - EBS/S3 volume cost for AMIs/images

How it works
============
Image builder
    -> Create EC2 instance based on configuration
        -> Create AMIs out of it.
            -> Test AMIs
                -> Distribute (Can be copied to multiple region for usage)

Configuring an Image Builder
============================
Image builder service
1. Create image builder pipeline
    - Build schedule Week/Day or CRON or Manual
        - Run by schedule/ Run when there is an update
2. Choose Recipe
    - New Recipe/ Existing one
    Image type
        - Output - AMI or Docker Image
    - Name and version
    - Source Image
        - Aws managed / Custom AMI
        - OS
        - Image origin - AWS/OwnByMe/SharedWithME
        - Select image ARN
        - OS Version options
    - root dir
    - Components
        - Software which we want to package in the AMI
        - Like aws cli, Java,..
        - Order of components installation
    - Test components
        - Test components are being configured to test the image after the build.
3. Infra configuration
    - To build the image using Image builder, the EC2 going to be used by image builder
        need to have few important permission that can be given via IAM roles
        * EC2InstanceProfileForImageBuilder
        * EC2InstanceProfileForImageBuilderECRContainerBuilds
        * AmazonSSMManagedInstanceCore
    - Choose the IAM role that has the above permissions. Or create one.
    - EC2 type
    - VPC subnet
4. Distribution settings
    - Configure Regions to distribute the output image to other region.

To run manually,
    - Our new Pipeline
        -> Actions
            -> Run pipeline

While image builder is building,
    we can see one EC2 machine started and which is used for image building.
    Once image is built, another ec2 machine used for testing the image.
        After building and testing EC2 machines will be terminated.
    And then image is distributed over configured regions.
    Now this new AMI should be available in AMIs/MyAMIs sections.
        (Image builder registers the new AMI ans it's available for us to use.)

EC2 instance store
==================
    - High performance hardware disk attached to the EC2 instance.
    - Provide great performance over EBS (which is attached through network and so have latency)
    - EC2 instance store (ephemeral) lose their data when stopped.
    - Good for buffer, temp data, cache.
    - Risk of data loss.
    - Backup and replication are our responsibility.

Elastic File System - EFS
=========================
    - Managed NFS(Network File System) that can be connected to 100s of EC2s at a time.
    - Works with Linux instance only
    - Can be used with Multiple ec2 across multiple AZs
    - high availability and durability
    - High cost - Pay for what you use
    - No capacity planning and scale up/down when required.
    - NFSv4. 1 protocol,which allows you to mount it like any other file system.
EBS vs EFS
==========
Regions and AZs
    EBS -
        specific to Regions and AZs.
        Means EBS volume attached to EC2 instance in one az cannot be (removed and)attached to another az's ec2.
        If needed , create a snapshot of EBS and copy to another az and attach to EC2.
        But that's gonna create another EBS volume.
        Replication is done within AZ
    EFS -
        One EFS can be mounted to multiple EC2 instances across AZs and regions.
        If one EC2 adds a file in the EFS, all other EC2s will able to see it.
        Not specific to region/az
        Replication is done across regions/AZs

EFS IA (EFS Infrequent Access)
==============================
* For files that are rarely accessed can be moved to EFS IA.
* Provides 90% cost optimization w.r.t. EFS standard.
* To enable, Create a lifecycle policy and attched it to the EFS.
    Based on the policy files will be moved to EFS IA by their last access time.
* When files in EFS IA accessed, they will be moved to EFS standard.
* EC2 doesn't know about any of these. For EC2, EFS or EFS IA both look the same.
    It's aws who does cost optimization work behind the screen.

Shared Responsibility model of EC2 storage
==========================================

AWS
====
* Infra
* Replication of EBS and EFS volumes
* Replace HW failure
* Data privacy

Our
===
* Backup and snapshot procedure.
* Data encryption
* Responsibility of Data on the drive.
* Risk of EC2 instance store

Amazon FSx
==========
* 3rd part file system
* Fully managed.
* Cost optimized and high available file system

1. FSx for windows server
2. FSx for lustre
3. FSx for netapp ONTAP

FSx for windows server
======================
* Windows native share file system
* Fully managed and high reliable
* Build on Windows file server
* Supports SMB and windows NTFS protocols
* Integrated with MS active directory
* Can be accessed from AWS or on premiss both possible.
Storage:
* It works with windows file server.

FSx for lustre
===============
* High performance computing linux file system
* Lustre - Linux and Cluster
* High workloads like machine learning, analytics, video processing, Financial modeling...
* Scales upto 100s of gb, millions of iops, sub-millies of latency
Storage:
* stores the data in S3 and can be accessed by on-premiss and ec2 instance
* Amazon Linux, Amazon Linux 2, Red Hat Enterprise Linux (RHEL), CentOS, SUSE Linux and Ubuntu.

ASG(Auto Scaling Group) and ELB (Elastic Load Balancer)
==========================================================
High Availability
    If there is any failure in one instance, the other instance should be taking care of the requests.
Scaling (Elasticity)
    When there is a hike in load, the backend systems should be capable of handling it.
    Vertical and Horizontal scaling
Agility
    Go global with Simple UI clicks.

Vertical
    - Increasing size capacity of the instance.
    - High availability cannot be done with Vertical scaling
Horizontal
    - Increasing the number of instances serving the requests.
    - High availability achieved.

ELB (Elastic Load Balancer)
===========================
* Load balancer
    -> security group to expose to outer world
    -> Select/ Create the target group.
        -> Name
        -> target group is List of EC2 instances.
        Todo : Check if this region specific?
            Is target group specific to region?

Type of LBs
* ALB
    - Suitable for optimal load for web servers
    - Http/Https2
* NLB
    - Ultra high performance
    - Support for TCP/UDP
* Gateway Load Balancer
    -
* Classical
    - Not used anymore.

Auto Scaling Group
===================
    - Only horizontal scaling can be done in auto scaling group.
    - When there is a need for more/less instances ASG detect it (based on the information it get from LB) and scale up/down
    - ASG can be enabled or disabled.
    - Min and max can be selected.

Strategies
==========
    - Manual
        -
    - Dynamic
        - Simple step - When reaching Upper/Lower trigger of CPU usage, increase/decrease number of instances
        - Target tracking - Always maintains the configured CPU usage
        - Schedule  -
    - Predictive scaling
        - Using ML algorithm scale up/down.


S3 (Simple storage service)
===========================
* Main building block of AWS
* Infinitely scalable.
* Backbone of Websites (js stored in S3 serves as Frontend through cloud front)
* Many AWS services use S3 as integration
    - EBS snapshots are stored in S3, but will not be visible. We will pay for EBS snapshot stored in S3.

Use cases
========
* Backup and storage
* Disaster recovery
* Archive
* Hybrid cloud storage
* Application hosting
* Media hosting
* Data lakes and big data analytics
* Software delivery
* Static website

Buckets
=======
* S3 can store objects(files) in buckets(dir)
* Bucket should be having unique name globally (across all region and all accounts)
* But buckets are defined in the region level.
* Naming conventions
    - No upper case
    - No underscore
    - 3 to 63 length
    - Not an IP
    - Must start with lower letter or number
* Each file store will have a key
    s3://bucket_name/file.txt
* There is no concept of directories/folders in S3
    s3://bucket_name/parent_folder/child_folder/file.txt
    Here, parent_folder/child_folder/ is just a prefix.
        S3 UI will make it look like a folder structure, but it's not.
* Each AWS account can create 100 buckets, though more are available by requesting a service limit increase.

Objects
========
Max size - 5TB (5k GB)
For uploads mores than 5GB, use multipart uploads
* Meta data - list of key value pair - System or user meta data
* Tags - Unique key value pair used for security/life cycle
* Version id (if versioning is enabled)

S3 security
===========
User based : IAM policies - which api calls should be allowed for a specific user
Resource based :
    Bucket policy : Bucket wide rules from the s3 console - allows cross account
    Object Access Control List : fine grains
    Bucket Access Control List : less common

An IAM principle can access the resource,
    If IAM permission allows
    ****** OR *******
    If IAM bucket policy allows
    ****** AND *******
    There is no explicit deny

Encryption:
    Encrypt objects in S3 using encryption keys (and make sure you can decrypt too)

Bucket Policy ** for exam
=============
* Allow the bucket to public : attach a bucket policy that provide public access
        So that anyone from the world can access the s3 object in the bucket.
* Allow the IAM user from same account of S3 : Provide IAM permission to access the S3, to restrict to bucket, Use IAM Policy
* Allow EC2 to use S3 : Attach IAM roles
* Allow the IAM user from outside account of S3 : Attach a bucket policy to allow the other account/users to access the bucket

Policy structure
================
* Json based policy
* Similar to IAM policy
{
    "version" : "2012-10-17",
    "Statements" : [
        {
            "Sid" : "PublicRead",
            "Effect" : "Allow",
            "Principle" : "*",
            "Action" : [
                "s3:GetObject"
            ],
            "Resource" : [
                "arn:aws:s3:::my-bucket/*"
            ]
        }
    ]
}

Resource - Buckets and objects
Action - List of APIs allowed
Effect - Allow or Deny
Principle - Account / User for which the policy is applied

Bucket Policies can used for
* Providing the public access
* Force the object encryption at upload
* Grant access to other account user to access the S3 Bucket and Objects

Access block settings
=====================
Block all public access

* If out bucket will never be exposed to public
    then all access setting can be ON
* If out bucket will be exposed to public
    then all access setting should be disabled
    And then only bucket policies can be applied.

* these are created for preventing data leake
* This settings can be set at account level.
    * If it's turned on at account level,
        all buckets in the account will have S3 access block settings turned on.

Create bucket policy
====================
Write manually using docs
    OR
Create using policy generator
=============================
Select type of policy (S3 bucket policy)
    -> Actions - Permissions
    -> Effect - allow/deny
    -> Resource - arn:aws:s3:::bucket-name/*
    -> Generate -> Will give a Policy JSON

S3 website
=====================
* S3 can host static websites and have them accessible on the www.
* The website URL will be :
    <bucket-name>.s3-website.<AWS region>.amazonaws.com
                                OR
                            -<AWS region>
* If you get a 403 Forbidden, make sure bucket policy allows public access.

How to set up the S3 website?
=============================
Go to S3 bucket
    -> Properties
        -> Look for static we hosting
            -> Edit it and enable static web hosting
                -> Provide index file name -> The file which will be served when there is a request.
                -> Optional - Provide error file name
                -> Optional - Provide redirection rules
            -> Save
Now the static website should be accessible.
URL will be like mentioned above,
 Also can be found in the Static web hosting section

S3 Versioning
=============
* Version the files whenever there is a change
* Enabled at the bucket level.
* Majorly used for web hosting static files, but can be applied to any object in s3.
* Benefits of using Versioning
    - Protect against unintended deletes
    - Easy roll back to previous version
* Any files that exist before versioning will have version as "NULL"
* Suspending versioning does not delete the previous versions.

How to enable versioning
* Bucket properties
    -> Enable versioning
* Whenever there is a upload a new version will be created.
* On clicking the List all version radio button
    We can see files and it's versions
* When deleting a specific version, it will be permanently deleted.
* When we delete a file at high level,
    a delete marker will attached to the file.

S3 server access logging
========================
* For audit purpose we want to log all access to s3 buckets.
* Any request made to s3 from any account that
    are authorized or denied will be logged into another logging bucket.
* These data can be analyzed using data analytics tools.
* Use cases:
    - For debugging
    - Audit
    - view suspicious pattern
    Majorly for security purposes

How to enable it?
Bucket Properties
    -> Server Access Logging
    -> Enable
        -> Configure the logging bucket where access logs will be stored.
* If you enable logging on multiple source buckets that identify the same target bucket,
    the target bucket will have access logs for all those source buckets.
* However, each log object reports access log records for a specific source bucket.

https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html

S3 Replication
==============
* When enabling the S3 Replication, files in the source bucket will be copied to target
* Must enable versioning - both source and target
* Types - * CRR - Cross Region Replication
          * SRR - Same Region Replication
* Buckets can be from different account.
    - The target needs to be attached with bucket policy that allows the source bucket owner account.
* Copying is async.
* Can attach a single source to multiple target buckets.

CRR - Used for compliance, low latency access, replication across accounts.
SRR - User for log aggregation, live replication between prod and test accounts

How to enable Replication?
* Before create a target bucket and enable versioning
* Enable versioning in the source bucket.
* Properties
    -> Replication
        -> Enable replication
        -> Configure destination bucket (can be from other account as well)
    -> Create.
* Files exist before the replication enabled will not be replicated.
    Only files after the replication will be enabled.

S3 Storage Classes
===================
* Standard
* Standard - infrequent access
* One zne - infrequent access
* Glacier Instant Retrieval
* Glacier Flexible Retrieval
* Glacier Deep archive
* Intelligent Tiering

When creating an object in s3,
    we can choose it's storage class
    and can modify manually after create.
Can use storage life cycle configuration to move the objects from one class to other

S3 Durability
=============
* Highly durable
* same for all storage classes

S3 Availability
===============
* How readily available to use.
* Varies depending upon the storage class
* Eg: S3 standard has 99.99% availability = not available 53mins a year

S3 Standard - general purpose
=============================
* 99.99% available
* Used for frequent access
* Low latency and high throughput.
* Sustain 2 concurrent aws facility failure.
* No retrieval changes
* Use cases:
    - Big data analytics
    - mobile and gaming purposes
    - CDN

S3 standard IA - Infrequently Accessed
======================================
* For data that is less frequently accessed but requires rapid access when needed.
* Lower cost than S3 standard.
* 99.99% available
* use case: Disaster recovery and backup

S3 One Zone - Infrequently Accessed
======================================
* Highly durable 99.999999%
* Data is lost when az is destroyed.
* 99.5% available
* Use case: Secondary backup - Used for data that can be created again.

GLACIER STORAGE CLASSES
========================
* Low cost object storage - used for archival and backup.
* Pricing : price for storage + cost of retrieval
Three classes
S3 Glacier Instant Retrieval
    - lowest cost storage, up to 68% lower cost (than S3 Standard-Infrequent Access),
    - Used for long lived data that is accessed only a quarter and requires milli seconds retrieval.
    - High durable, high available and low latency
    - Lower cost for per gb storage and higher cost for per gb retrieval.
    - data is stored across multiple physically separated AWS Availability Zones in a given year.
    - Min storage duration 90 days

S3 Glacier Flexible Retrieval
    - Low cost than 10% lower of Instant retrieval
    - Used for object retrieved 1-2 times a year.
    - Async retrieval
    - Has 3 flexibility
        - Expedited- 1 - 5 mins
        - Standard - 3 to 5 hours
        - Bulk - 5 to 12 hours - Free retrieval
        - Min 90 days storage duration
S3 Glacier Deep archival
    - Standard 12 hours and bulk 48 hours
    - Min storage duration is 180 days.
    -  up to 75% lower cost (than S3 Glacier
https://aws.amazon.com/s3/storage-classes/glacier/

S3 Intelligent tiering
========================
* Small monthly monitoring and auto tiering fee.
* Moves objects automatically to Access tiers based on the usage.
* No retrieval changes

Access tier types
    - Frequent Access Tier (auto): default
    - Infrequent Access Tier (auto): Objects not accessed for 30 days.
    - Archive Instant Access Tier (auto): Objects not accessed for 90 days.
    - Archive Access Tier (auto): configurable from 90 days to 700+ days.
    - Deep Archive Access Tier (auto): configurable from 180 days to 700+ days.

LifeCycle Configuration
=======================
* Can be applied to entire bucket or specific objects using filters

Options:
========
* Transition current versions of objects between storage classes
    - Storage transition and days after object creation

* Transition previous versions of objects between storage classes
    - Storage transition and days after object creation

* Expire current versions of objects
    - days after object creation

* Permanently delete previous versions of objects
    - days after object creation

* Delete expired delete markers or incomplete multipart uploads
    - days after object creation

S3 Object Lock & Glacier Vault lock
===================================
S3 Object Lock
* WORM model (Write once Read Many)
* Block the object for specific amount of time.

